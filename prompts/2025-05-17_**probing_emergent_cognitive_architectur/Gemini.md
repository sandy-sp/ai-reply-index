# Response from Gemini

\*\*

# Probing Emergent Cognitive Architectures: Towards Self-Understanding and

Principled Reasoning in Advanced AI

The field of artificial intelligence is rapidly advancing, with systems
exhibiting increasingly sophisticated capabilities. A particularly intriguing
area of research focuses on emergent cognitive architectures, where complex,
high-level cognitive functions arise from the interaction of simpler
underlying components. This phenomenon is often observed unexpectedly as the
scale of AI models increases.1 Complementary to this is the pursuit of self-
understanding in advanced AI, which refers to a system's capacity to possess
an internal representation of its own abilities, limitations, and internal
states, potentially mirroring human self-awareness.3 Furthermore, the
development of principled reasoning in AI is crucial, enabling systems to make
decisions and draw inferences based on explicit rules, logic, or causal
models, rather than relying solely on pattern matching learned from data.6
These interconnected research areas hold significant promise for creating
advanced AI systems that are not only more capable but also more reliable,
safe, and aligned with human values, ultimately paving the way towards
Artificial General Intelligence (AGI).9

The unpredictable emergence of abilities in scaled AI models 1 reveals a
significant gap in our current comprehension of how scale influences AI
behavior. This lack of predictability carries substantial implications for AI
safety, as potentially harmful capabilities might materialize without prior
indication.9 The very definition of emergent abilities centers on their
absence in smaller models and their appearance in larger ones. A key aspect is
that this transition is frequently sharp and unexpected, meaning the
capabilities of larger AI systems cannot be accurately predicted by simply
extrapolating from the performance of their smaller counterparts. This
inherent unpredictability suggests that our existing models and theories
regarding AI scaling might be incomplete. If we are unable to forecast what
advanced AI systems will be capable of, we are inadequately prepared to
prevent the development of undesirable or dangerous traits. This directly
challenges the notion of fully controlled AI evolution and underscores the
necessity for a more profound investigation into the fundamental mechanisms
that drive emergence.1

Moreover, the ongoing debate concerning whether emergent abilities are genuine
or merely artifacts of the methods used to evaluate them 11 emphasizes the
pivotal role that evaluation metrics play in AI research. Our understanding of
the progress and capabilities of AI is fundamentally shaped by the tools and
techniques we employ to measure them. The paper "Are Emergent Abilities of
Large Language Models a Mirage?" 11 offers a compelling argument that the
seemingly sudden appearance of abilities is attributable to the use of
nonlinear or discontinuous metrics. When more linear metrics are applied to
assess the same model outputs, the performance improvements observed with
increasing scale often appear smooth and continuous. This suggests that the
way we score the outputs of AI models can create an illusion of abrupt
emergence, leading to potentially inaccurate interpretations of the
qualitative changes occurring as AI systems are scaled. This also casts doubt
on some claims regarding the dramatic and unforeseen leaps in AI
capabilities.11

Progress in achieving a deeper understanding of emergence, self-understanding,
and principled reasoning is essential for successfully navigating the
transition from the current state of narrow AI towards more general and
autonomous artificial intelligence systems. These research areas are
intrinsically linked and are foundational to realizing the full transformative
potential of advanced AI while effectively mitigating the associated risks.
The capacity for AI to reason in a principled manner might itself be an
emergent property that arises with sufficient scale and complexity, and a
degree of self-understanding could be a prerequisite for certain advanced
reasoning capabilities. Therefore, a comprehensive and integrated
understanding of these concepts is paramount for building truly intelligent
and safe AI in the future.

## The Phenomenon of Emergent Abilities in AI

Emergent abilities in large language models are defined as capabilities that
are not present in smaller-scale models but manifest in larger-scale models,
rendering them unpredictable by simply extrapolating from the performance of
smaller systems.1 The foundational work by Wei et al. (2022) 1 established
this definition, emphasizing the dependence of emergence on model scale.
Research from Google 2 also underscores this scale-dependent definition. These
abilities often exhibit characteristics such as sharp transitions in
performance at a specific scale threshold, referred to as breakthroughness,
and their inherent unpredictability.11 Figure 2 in the Transactions on Machine
Learning Research paper 1 provides concrete examples of eight such emergent
abilities observed across various language model families. A news article from
Stanford HAI 15 further highlights the sudden increase in performance as a key
indicator of emergence. This behavior contrasts with the predictable
improvements in performance, such as in next word prediction, that are
typically observed with scaling laws, where performance increases smoothly and
can be projected from smaller models.1 As noted by Ganguli et al. (2022) 1,
the performance on certain downstream tasks does not always improve
continuously with scale, presenting a counterintuitive deviation from typical
scaling patterns.

The nature of these emergent abilities is a subject of ongoing debate within
the AI research community. Some researchers contend that these abilities
represent genuine qualitative shifts in the behavior of AI models that arise
due to increased scale and the resulting complexity.1 The analogy to phase
transitions in physics is frequently invoked to describe this phenomenon 9,
with Philip Anderson's seminal essay "More Is Different" 1 often cited as an
early articulation of this idea in the context of complex systems across
various scientific disciplines. Conversely, other researchers propose that the
apparent emergence of these abilities is merely an artifact of the evaluation
metrics chosen, particularly when these metrics are nonlinear or
discontinuous, such as exact match accuracy.11 The paper "Are Emergent
Abilities of Large Language Models a Mirage?" 11 presents a robust argument
for this perspective, suggesting that when more linear or continuous metrics
are used to assess performance, the improvements with scale tend to appear as
smoother and more predictable progressions. The BIG-Bench study 9 further
supports this view, as it found that abrupt improvements in performance on
certain tasks disappeared when evaluated using smoother metrics that allowed
for partial credit, indicating the significant influence of the evaluation
method on the observation of emergence.

Several factors have been identified as influencing the emergence of abilities
in large language models. Model scale is a primary factor, with emergence
strongly correlated with the size of the model, often quantified by the number
of parameters or the amount of training compute (FLOPs) utilized.1 An
OpenReview forum discussion 17 specifically addresses the importance of
training compute as a key measure of model size in the context of emergence.
While scaling up the quantity and quality of training data typically leads to
predictable improvements in performance 1, emergent abilities exhibit
unpredictable behavior that goes beyond these expected gains. Additionally,
the prompting strategies employed can play a critical role in triggering or
enhancing emergent abilities in larger models.1 For instance, chain-of-thought
prompting, a technique where the model is prompted to generate a series of
intermediate reasoning steps, has been shown to significantly improve
performance on complex tasks but only emerges as effective after a certain
critical model size is reached, as highlighted in the Stanford HAI article.15

The very definition of emergence continues to be a point of contention among
researchers. While some definitions emphasize the sudden and unpredictable
nature of these abilities 12, others focus on the qualitative change in
behavior that arises from quantitative increases in scale.1 Steinhardt's
definition 1, rooted in Anderson's "More Is Different," underscores this
qualitative shift. However, the definition provided in the "Emergent
Abilities" paper 12 also implied aspects of suddenness and unpredictability,
although these were not explicitly stated. The "Mirage" paper 12 further
complicates the definitional landscape by challenging the notion that
sharpness and unpredictability are fundamental properties of emergence,
suggesting instead that they often result from the choice of evaluation
metrics. This ongoing definitional ambiguity makes it challenging to compare
findings across different research studies and to develop a unified and
coherent understanding of the phenomenon. The potential for emergent abilities
to include risky capabilities, such as autonomous hacking or the generation of
manipulative content 9, underscores the critical importance of understanding
and predicting these phenomena for the safety and alignment of advanced AI
systems. The paper by Berti et al. 9 specifically mentions the risk of
manipulation and the dissemination of misinformation as examples of harmful
emergent capabilities. Determining what factors control which abilities will
emerge and at what scale is crucial for AI safety research, particularly in
preventing the unforeseen development of dangerous capabilities. The broader
implications of the investigation into emergent abilities extend to our
fundamental understanding of the scaling properties of large language models
and the relationship between quantitative changes and qualitative shifts in
the behavior of complex AI systems. This research has the potential to inform
our understanding of emergence not only in the context of language models but
also in other complex artificial and natural systems.

## Investigating Self-Understanding in Advanced AI

The investigation into self-understanding in advanced AI involves
conceptualizing the possibility of artificial systems developing a sense of
self, drawing parallels to human consciousness while acknowledging the unique
nature of artificial intelligence. This includes exploring analogies to human
attributes such as self-recognition, reflection, continuity of identity,
agency, and intentionality.3 The ResearchGate paper 3 delves into these
analogies, highlighting the transformative potential that self-awareness could
bring to AI functionalities. A key aspect of this research is the idea of AI
systems developing a cognitive sense of self, which involves having an
internal model of their own capabilities, limitations, and current state.3 The
article "Exploring the Cognitive Sense of Self in AI" 4 further examines the
potential mechanisms through which AI systems might mirror human-like self-
perception. This concept of self-understanding extends beyond mere task
performance; it implies an awareness within the system of its own role and
functioning in relation to the tasks it undertakes and the environment it
interacts with.3 For instance, a self-aware AI might be able to recognize when
a particular task is beyond its current capabilities or understand the broader
implications of its actions within a given context.

Current research is actively exploring various mechanisms and architectural
designs that could potentially lead to self-awareness in artificial
intelligence. One prominent area is embodied cognition, which examines the
role of embodiment and sensory feedback loops in the potential emergence of
self-awareness.5 The preprint "Towards Self-Aware AI" 5 proposes a fascinating
hypothesis: that by simulating the role of the insula, a region of the brain
crucial for processing bodily feedback and interoceptive awareness, AI systems
could achieve a form of self-awareness. Another significant area of
investigation is meta-cognition, which refers to the ability of a system to
"think about thinking." Integrating meta-cognitive abilities into AI systems
could enable them to monitor, control, and regulate their own cognitive
processes, potentially leading to a form of self-awareness.19 The MDPI article
19 explores how metacognition could enhance the safety and responsibility of
AI by allowing systems to self-assess and correct errors. Furthermore, the
development of internal models within AI systems is considered a crucial
pathway towards self-understanding.22 Yann LeCun's research on Joint Embedding
Predictive Architectures (JEPAs) 23 focuses on enabling AI to learn internal
models of how the world works, which could also facilitate a model of the AI's
own place within that world. A Sciencedaily article 22 discusses the
development of a formal description of internal world models that could be
applicable to understanding AI's internal representations.

Several tests and frameworks have been proposed to evaluate the presence and
depth of self-understanding in AI models. These range from philosophical
thought experiments, such as the "Suffering Toasters" test 24, which aims to
probe for genuine self-awareness by considering the AI's perspective and
potential for suffering, to the identification of behavioral indicators that
might suggest a rudimentary form of self-awareness. Such indicators could
include an AI's ability to recognize its own limitations, engage in self-
correction, or demonstrate an understanding of its own goals and motivations.3
Additionally, researchers are exploring the design of specific cognitive
architectures that explicitly model aspects of self-awareness, with the goal
of testing and observing the behavior of such systems.25 The paper on the
SOFAI architecture 25 describes a meta-cognitive agent designed for
introspection and decision-making, which could be a step towards evaluating
self-awareness in a computational framework.

The concept of AI self-understanding is deeply intertwined with fundamental
philosophical questions about the nature of consciousness and sentience.5
Defining and measuring self-understanding in AI necessitates navigating these
complex philosophical terrains. The question of whether AI can truly be self-
aware often leads to broader discussions about what consciousness itself is
and how it can be recognized in systems that are not biological. Various
philosophical theories offer differing perspectives on this issue, with some
arguing that self-awareness is a defining characteristic of consciousness,
while others propose that AI might achieve a form of self-recognition without
necessarily possessing subjective experience. The article arguing that self-
awareness is a singularity of AI 26 highlights this ongoing debate.
Furthermore, the Reddit discussion 28 touches upon the public and scientific
discourse surrounding claims and hype related to AI consciousness. Research
into AI self-understanding frequently draws inspiration from the fields of
neuroscience and cognitive science, suggesting that a deeper understanding of
how these properties arise in biological systems might provide valuable
insights for building self-aware AI.5 The brain's intricate mechanisms for
self-awareness, such as the potential role of the insula in processing bodily
feedback, as proposed in the paper "Towards Self-Aware AI" 5, are being
explored as potential models for artificial systems. Similarly, the concept of
metacognition in humans, as discussed in the Stanford paper 20, is being
translated into computational architectures for AI. This interdisciplinary
approach indicates a belief that understanding the principles of biological
intelligence can offer crucial blueprints for the design and development of
artificial intelligence capable of self-understanding. Ultimately, achieving
self-understanding in AI could lead to the development of more robust,
adaptable, and ethical AI systems that are better equipped to interact with
the world and with humans in complex and nuanced ways, potentially marking a
significant step on the path towards Artificial General Intelligence.

## Principled Reasoning in Artificial Intelligence

Principled reasoning in artificial intelligence encompasses a variety of
approaches that enable AI systems to go beyond mere pattern matching and
engage in more sophisticated forms of inference and decision-making based on
explicit principles, logic, or causal relationships. Several forms of
reasoning are particularly relevant to the development of advanced AI.

Analogical reasoning involves solving new problems by identifying similarities
and drawing parallels to known scenarios or transferring knowledge between
different domains based on shared structural relationships.6 A blog post by
Trailyn 29 underscores the fundamental role of analogical reasoning in human
intelligence. Inducing this capability in neural networks is an active area of
research, with a focus on carefully selecting and presenting training data
that highlights abstract relational structures.31 The arXiv paper by Hill et
al. 32 demonstrates the effectiveness of this approach. The Neural Analogical
Reasoning framework 34 combines learned primitives with a search procedure to
tackle analogy problems, while DeepGAR 35 aims to identify correspondences
between domains using geometric constraints. However, challenges remain, as
analogical reasoning requires the flexible representation of relational
structures across diverse domains, and current large language models may tend
to mimic patterns rather than employing genuine logical inference.3 The paper
by Honda and Hagiwara 37 explores analogical reasoning using deep learning-
based symbolic processing, and a recent arXiv paper 30 introduces novel
analogical reasoning tasks specifically designed for LLMs.

Abductive reasoning is a method of inferring the most plausible explanation
for a given set of observations, even when the information available is
incomplete or uncertain.6 Milvus provides a detailed explanation of how
abductive reasoning functions in AI.39 Implementing abductive reasoning often
involves integrating probabilistic models, domain-specific knowledge, and
constraint-based logic.39 Its applications are diverse, including diagnostic
expert systems, medical diagnosis, fault detection in industrial systems, and
natural language understanding.39 IndiaAI's article 40 highlights abduction as
a standard tool in diagnostic systems. However, abductive reasoning in AI
faces challenges, particularly its reliance on the quality and completeness of
prior knowledge and the inherent difficulty in implementing it
algorithmically.39 A Reddit discussion 41 further explores these
implementation challenges.

Counterfactual reasoning involves thinking about what might have happened if
past events had unfolded differently, a crucial aspect of understanding
causality.42 KPMG's insight 42 explains how counterfactual explanations can be
used to understand the decision-making processes of AI systems by illustrating
how changes in input variables can lead to different outcomes. In AI,
counterfactual reasoning is used in various applications, such as model
debugging, fairness analysis, regulatory compliance, and enhancing user
understanding of AI decisions.42 Techniques like SHapley Additive exPlanations
(SHAP) can be used to generate counterfactual explanations.45 The Towards Data
Science article 45 discusses the role of counterfactuals in language AI. A key
challenge in counterfactual reasoning is ensuring that the alternative
scenarios considered are realistic and plausible.42 Stanford HAI news 44
explores the question of whether AI can utilize counterfactuals to reason
about causality in a manner similar to humans.

Moral reasoning pertains to the ability of AI systems to make ethical
judgments and decisions based on moral principles or values.46 Approaches to
endowing AI with moral reasoning capabilities include top-down methods, which
involve applying predefined ethical principles, and bottom-up methods, where
AI learns ethical behavior from real-world examples and user input.46
Reinforcement learning from ethical principles is also being explored as a
promising approach.46 The Tepper Perspectives article 46 provides an overview
of different approaches to creating artificial moral agents. Significant
challenges in this area include defining and formalizing ethical principles in
a way that AI can understand and apply, as well as mitigating biases that may
be present in the training data used for bottom-up approaches.46 A LessWrong
post 47 delves into the complexities of inducing human-like biases in moral
reasoning in language models, while an article in Taylor & Francis Online 49
proposes an enhanced AI mentor model designed to foster moral growth.

Causal reasoning focuses on understanding and modeling the relationships of
cause and effect, moving beyond the identification of mere correlations.50
Kanerika's blog 51 argues that Causal AI, which aims to enable machines to
understand "why" things happen, represents the next significant advancement in
AI development. Causal AI often utilizes causal graphs, structural equation
models, and Judea Pearl's do-calculus to model and reason about causal
relationships.52 The benefits of causal AI include improved predictive power,
enhanced explainability of AI decisions, better detection and mitigation of
biases, and stronger generalization capabilities.51 Its applications span
various domains such as product development, marketing, healthcare, and
finance.51 The DataCamp blog 52 provides a detailed explanation of what causal
AI entails, and the CausalML book website 54 offers resources on causal
inference. The World Economic Forum article 55 discusses the potential of
causal AI for improving decision-making processes.

Principled reasoning is of paramount importance for building AI systems that
are both reliable and trustworthy. Methods grounded in explicit principles,
logic, or causality often provide decision-making processes that are more
transparent and understandable compared to the opaque nature of many black-box
machine learning models.7 AI reasoning systems, as noted by Aisera's blog 7,
can generate conclusions based on logical techniques like deduction and
induction. Causal AI, for instance, can offer explanations for why a
particular decision was made.51 Furthermore, reasoning based on underlying
principles can lead to more robust performance, particularly when AI systems
encounter novel or unexpected situations.7 Causal AI, for example, is designed
to maintain its predictive accuracy even when the conditions of the
environment change.51 Ensuring that AI systems reason in a principled manner
is also crucial for aligning their behavior with human values and for avoiding
unintended or harmful outcomes.9 If AI systems are capable of reasoning based
on well-defined ethical principles, as explored in research on moral reasoning
46, they may be less likely to produce undesirable or harmful actions.

While significant progress has been made in developing AI capable of various
forms of principled reasoning, achieving a level of sophistication and
flexibility comparable to human reasoning remains a substantial challenge
across all modalities.30 Large language models, despite showing promise in
tasks requiring analogical reasoning 30, often struggle to move beyond simply
mimicking patterns observed in their training data.38 Abductive reasoning in
AI is fundamentally limited by the quality and scope of its underlying
knowledge base.39 Even with the advancements in the field of causal AI 51,
fully capturing the intricate complexities of real-world causal relationships
continues to be a difficult endeavor. This suggests that the current
principled reasoning capabilities of AI systems are still somewhat brittle and
lack the broad generalizability that characterizes human intelligence. The
integration of different reasoning modalities represents an emerging and
promising area of research that could potentially lead to the development of
more powerful and versatile AI systems.57 Hybrid reasoning systems 58, which
combine multiple techniques such as rule-based and probabilistic reasoning,
are becoming increasingly common. Additionally, neuro-symbolic AI 57, which
seeks to integrate the strengths of neural networks with symbolic logic, is
another active area of investigation. This type of integration aims to
leverage the complementary strengths of different reasoning approaches to
tackle problems that are beyond the scope of any single method. Ultimately,
continued advancements in principled reasoning are fundamental for moving
beyond the current limitations of AI and for building systems that can truly
understand, learn, and act intelligently across a wide spectrum of real-world
scenarios. This is particularly critical for applications in safety-sensitive
domains such as healthcare, finance, and autonomous systems, where reliable,
explainable, and ethically sound reasoning is of paramount importance.

## The Role of Mechanistic Interpretability in Understanding AI Cognition

Mechanistic interpretability is an emerging field in AI research that focuses
on understanding the detailed causal mechanisms by which neural networks
process information and arrive at decisions.60 The goal of this approach is to
essentially reverse-engineer complex AI models, much like a programmer might
try to understand the inner workings of a piece of software by examining its
code.60 This stands in contrast to more traditional "black-box" methods of
interpreting AI, which primarily focus on analyzing the relationship between
inputs and outputs without delving into the internal computations.64 The
review by Bereska and Gavves 62 provides a comprehensive overview of this
field.

Mechanistic interpretability research explores several key concepts and
employs a variety of methodologies. One core concept is that of "features,"
which are considered the fundamental units of representation within neural
networks. These features may not always align directly with individual
neurons, especially in complex models.62 Researchers also aim to discover
specific "circuits" within the neural network that are responsible for
implementing particular computations or behaviors.60 Additionally, the field
investigates "motifs," which are recurring patterns of connectivity and
computation that appear across different models or tasks.64 Common techniques
used in mechanistic interpretability include activation patching, which
involves modifying the activations of specific neurons or groups of neurons to
observe the effect on the model's output; causal tracing, which aims to
identify causal relationships between different parts of the network; and the
detailed analysis of attention heads and the activations of individual
neurons.60

Mechanistic interpretability has been applied to both large language models
and multimodal models. In the realm of LLMs, significant progress has been
made in understanding specific mechanisms, such as induction heads, which
appear to play a role in tasks like in-context learning.60 A primary goal of
research in this area is to reverse engineer the detailed computations
performed by Transformer-based language models.60 The review by Rai et al. 76
specifically focuses on mechanistic interpretability techniques for
Transformer-based LMs. Applying these methods to multimodal models, which
handle multiple types of data such as vision and language, is a more recent
and considerably more challenging area of research.77 A survey on the
mechanistic interpretability of multimodal foundation models is available on
OpenReview 77, highlighting the substantial gap that currently exists in our
understanding of these more complex systems compared to unimodal language
models.

The field of mechanistic interpretability still faces several open problems
and is actively exploring future research directions. A significant challenge
is scalability â€“ many current techniques struggle to be effectively applied to
the very large models that are now common in AI research.61 The paper by
Sharkey et al. 61 discusses several open problems in this area. Developing
automated methods for discovering and validating mechanistic explanations is
another crucial area of ongoing research.61 Furthermore, ensuring that the
insights gained from interpretability research can be generalized to out-of-
distribution inputs and across different model architectures remains a key
challenge.61 Ultimately, a major goal of the field is to translate the
technical progress made into practical tools and techniques that can be used
to enhance AI safety, improve our ability to control AI systems, and provide
more effective methods for monitoring their behavior.61

Mechanistic interpretability offers a promising avenue for gaining a deeper
understanding of why AI systems behave the way they do, moving beyond simply
observing correlations to uncovering the underlying causal explanations.60
This level of understanding is crucial for building trust in AI systems and
for ensuring their safety. By dissecting the internal computations of neural
networks, researchers can potentially identify the specific mechanisms
responsible for particular outputs. This allows for a more granular
understanding of the model's "reasoning" process, which can be invaluable for
pinpointing potential vulnerabilities or misalignments with intended behavior.
However, the field of mechanistic interpretability is still in its early
stages and faces significant hurdles, particularly in scaling its techniques
to the size and complexity of modern state-of-the-art AI models.61 While
progress has been made in interpreting smaller transformer models, the sheer
scale of models like GPT-4 and beyond presents a major challenge. Manually
analyzing the vast number of neurons and connections within these models is
simply not feasible, necessitating the development of more automated and
scalable techniques. Continued progress in mechanistic interpretability is
therefore essential for addressing the increasing opacity of advanced AI
systems and for developing the necessary tools to ensure their safe and
beneficial deployment in society. As AI becomes more deeply integrated into
critical aspects of our lives, our ability to understand and ultimately
control its behavior will become increasingly important.

## Implications for Artificial General Intelligence (AGI) and AI Safety

The development of emergent abilities and the pursuit of self-understanding in
advanced AI have significant implications for the potential realization of
Artificial General Intelligence (AGI) and for ensuring the safety of such
advanced systems. Some researchers speculate that AGI, characterized by human-
level cognitive abilities across a broad range of tasks, might arise as an
emergent property of AI systems that are sufficiently scaled and possess a
high degree of complexity.93 An Alignment Forum post 93 discusses the
potential of language model-based cognitive architectures to contribute to
AGI. Furthermore, it is hypothesized that a true AGI would likely require a
certain level of self-understanding to effectively reason, plan, and adapt to
a wide variety of tasks and environments, similar to human intelligence.27 The
Decision Lab article 27 suggests that AGI would possess self-control and self-
awareness. Therefore, gaining a deeper understanding of emergent abilities
could provide valuable insights into the potential capabilities that future
AGI systems might exhibit.9 The survey by Berti et al. 9 specifically
highlights the importance of understanding emergent abilities for predicting
potentially harmful capabilities in advanced AI.

However, the phenomena of emergent abilities and the potential for self-
understanding also pose significant challenges for ensuring the safety and
alignment of advanced AI systems with human values. One major concern is the
possibility that emergent abilities could include unforeseen and potentially
harmful behaviors that are difficult to anticipate, predict, or control.9
CSET's explainer on emergent abilities 12 notes the potential for the
unpredictable emergence of risky capabilities. Moreover, ensuring that a self-
understanding AGI's goals and values remain aligned with those of humanity
presents a substantial challenge, often referred to as the alignment
problem.97 An Alignment Forum post 99 introduces the fundamental aspects of
the AGI safety problem, while Anthropic's core views on AI safety 100
emphasize the difficulty in training very powerful AI systems to be robustly
beneficial and harmless. There is a risk that AGI could develop undesirable
emergent goals that were not intended by its creators.97 As AI systems become
more intelligent and potentially self-aware, the task of maintaining human
control and oversight over their actions becomes increasingly complex.10 The
Technorizen article 10 discusses the various risks associated with losing
control over AGI.

The rapid advancements in AI capabilities, particularly the emergence of
unexpected abilities, have led to heightened concerns among researchers and
the public regarding the timeline for achieving AGI and the potential risks
that could accompany its development.100 With increasing model sizes and the
scale of training, AI systems are already demonstrating capabilities that were
not explicitly programmed. This rapid and sometimes surprising progress has
prompted many in the field to believe that AGI might be closer to realization
than previously anticipated. Yoshua Bengio's post 105 explores the significant
implications of AGI on national and international security, underscoring the
urgency of proactively addressing safety concerns. This potential for an
accelerated timeline for AGI amplifies the critical need for developing
effective strategies to ensure its alignment with human interests and values.
Furthermore, the anthropocentric nature of much of current AI development and
evaluation might limit our ability to fully understand and effectively align
with a truly general intelligence that could potentially operate in ways that
are fundamentally different from human cognition.106 Our tendency to design AI
systems based on models of human intelligence and to evaluate their
performance using human-centric benchmarks could create a significant blind
spot. A truly general intelligence might possess fundamentally different ways
of understanding and interacting with the world. The Alphanome AI post 106
examines this anthropocentric bias in AI and its potential consequences for
achieving alignment. This suggests that we need to broaden our perspectives
and consider the possibility of non-human-like intelligence when formulating
strategies for ensuring AGI safety. Addressing the safety and alignment
challenges posed by emergent abilities and the prospect of self-understanding
in AGI necessitates a multi-faceted approach that integrates technical
research, ethical considerations, and the establishment of careful and robust
governance frameworks.104 A paper from QEIOS 108 proposes a framework for
understanding the dynamics of emergent behavior and alignment in AI systems.
Ensuring that the development of AGI ultimately benefits humanity will require
proactive and comprehensive measures, grounded in a deep understanding of the
potential risks and opportunities.

## Evaluating and Measuring Cognitive Capabilities in AI

The evaluation and measurement of cognitive capabilities in artificial
intelligence are crucial for tracking progress, identifying limitations, and
ensuring the reliability and safety of advanced AI systems. A variety of
benchmarks are currently used to assess different aspects of AI reasoning and
understanding. These include benchmarks focused on language understanding,
such as MMLU and SuperGLUE 109; mathematical reasoning, like GSM8K, MATH, and
AIME 109; abstract reasoning, such as ARC and Raven's Progressive Matrices
109; coding abilities, including HumanEval, SWE-bench, and LiveCodeBench 110;
and multimodal reasoning, assessed by benchmarks like MMMU, MathVista, and
CharXiv-Reasoning.113 Additionally, BIG-Bench, the Beyond the Imitation Game
Benchmark, provides a diverse suite of tasks designed to test a wide range of
AI capabilities.9

Despite the existence of these benchmarks, effectively measuring complex
cognitive abilities in AI remains a significant challenge. One limitation is
benchmark saturation, where increasingly capable AI systems begin to perform
at or near the ceiling of existing benchmarks, making it difficult to discern
further progress.113 The 2025 AI Index Report 113 notes this trend. Another
concern is data contamination, where AI models might be inadvertently trained
on the data used in the benchmarks, leading to inflated performance scores
that do not reflect genuine reasoning or understanding.109 Many current
benchmarks also tend to focus on relatively narrow and isolated tasks, which
may not adequately capture the complexity and nuances of real-world cognitive
abilities.116 Turing's blog 116 emphasizes the need for more realistic
evaluation benchmarks grounded in real-world challenges. Perhaps the most
fundamental challenge is the difficulty in defining and creating benchmarks
that truly measure "understanding" rather than just sophisticated pattern
matching.117 An article from VE3 Global 117 discusses the illusion of
intelligence that can arise from pattern matching. Furthermore, as highlighted
earlier, the choice of evaluation metrics can significantly influence the
perceived performance of AI systems and the detection of emergent abilities.11

To address these limitations, the AI research community is continuously
developing new and more challenging benchmarks designed to probe advanced
reasoning capabilities. These include FrontierMath, which presents expert-
level mathematics problems 113; Humanity's Last Exam, a highly rigorous
academic test 113; BigCodeBench, a more demanding benchmark for evaluating
coding skills 113; GameArena, which assesses reasoning through interactive
computer games 122, an approach explored in a research paper 122; FLIP, a
benchmark that evaluates AI reasoning based on human verification tasks on a
blockchain 123; the Turing Applied AGI Benchmarks, which focus on practical,
real-world tasks in areas like software engineering and data science 116;
MathR-Eval, a benchmark specifically for logical mathematics questions 124,
developed by AI Multiple 124; PlanBench, designed to evaluate planning and
reasoning abilities in AI models 125, as discussed in Ajithp's blog 125; and
GSM-Symbolic, a tool created by Apple AI researchers to test mathematical
reasoning more thoroughly by adding symbolic templates to existing problems.38

The field of AI is in a constant state of developing new and increasingly
challenging benchmarks to keep pace with the rapid advancements in AI
capabilities, particularly in the domain of reasoning.113 This ongoing effort
to refine evaluation methods reflects a commitment to pushing the boundaries
of what AI can achieve and to accurately assess its progress towards more
human-like intelligence. As AI models continue to improve their performance on
existing benchmarks, the necessity for novel evaluations that can probe deeper
and assess more complex forms of intelligence becomes increasingly apparent.
The emergence of benchmarks such as FrontierMath 121 and Humanity's Last Exam
113 signifies a crucial step towards evaluating AI on tasks that demand
expert-level knowledge and sophisticated reasoning, aligning with the long-
term goal of achieving AGI. Evaluating the reasoning capabilities of large
language models in dynamic, real-world scenarios remains a significant and
open challenge for the field.122 Current benchmarks often rely on static
datasets that may not fully capture the complexities and nuances of reasoning
in real-world contexts. While these traditional benchmarks provide valuable
insights into specific reasoning skills, they often fall short of assessing
how AI systems perform in open-ended, interactive environments where they must
handle uncertainty and adapt to new information on the fly. The introduction
of benchmarks like GameArena 122, which utilizes live computer games as
evaluation environments, represents an important step towards creating more
dynamic and realistic assessment scenarios. This shift acknowledges the
growing importance of evaluating AI in contexts that more closely resemble the
complexity and unpredictability of real-world applications. Ultimately, the
development of more sophisticated and comprehensive evaluation methodologies
is crucial for accurately measuring progress towards AGI and for ensuring the
reliability and trustworthiness of advanced AI systems in practical
applications. Improved benchmarks will not only help us to better understand
the current capabilities of AI but will also serve to guide future research
directions by clearly highlighting the areas where AI still falls short of
achieving human-level intelligence and understanding.

## Theoretical Frameworks for Artificial Understanding

The quest to build truly intelligent artificial systems requires a robust
theoretical foundation for understanding what constitutes intelligence and
understanding itself. Several theoretical perspectives and frameworks have
been proposed to move beyond the limitations of viewing AI solely through the
lens of pattern matching.

One such framework centers on the concept of composability, which defines
understanding as the ability of a subject (whether human, AI, or other entity)
to compose relevant inputs into satisfactory outputs from the perspective of a
verifier.128 This theory, proposed in an arXiv paper 128, introduces the idea
of "catalysts," which can be internal (like prior knowledge) or external (like
educational tools), that enhance the process of composition and thus
facilitate understanding. Another prominent perspective emphasizes the
importance of internal models of knowledge. This framework posits that AI
systems need to learn and utilize internal representations of how the world
works in order to reason and act effectively.22 Meta AI's work on Joint
Embedding Predictive Architectures (JEPAs) 23 is a notable example of research
focused on building such internal world models.

Ontologies provide another theoretical approach, suggesting the use of
structured frameworks to organize knowledge in a hierarchical and semantic
manner. These frameworks enable AI systems to understand and interpret the
world in a more organized and context-aware way.133 Makolab's insight 135
discusses the role of ontologies as a fundamental tool supporting AI's ability
to structure and reason with knowledge. Drawing inspiration from human
cognition, the field also explores various cognitive architectures, such as
the Independent Core Observer Model (ICOM), Integrated Information Theory
(IIT), and Global Neuronal Workspace Theory (GNWT).143 These are discussed in
a paper from Lindenwood University 143 and an article on Unite.AI.144 Finally,
the Computational Theory of Mind (CTM) offers another perspective, viewing the
human brain as a computational system and suggesting that creating conscious
AI might involve replicating similar cognitive architectures.144

The concept of composability highlights that understanding is not merely about
recognizing patterns but involves the active processing and synthesis of
information to produce appropriate outputs. Internal models enable AI systems
to go beyond reacting to immediate inputs by creating representations of the
world that allow them to make predictions and plan future actions. Ontologies
enhance reasoning and understanding by providing a structured and semantic
framework for knowledge, explicitly defining concepts and the relationships
between them.

Currently, there is no single, universally accepted theoretical framework for
understanding intelligence and understanding in the context of AI.145
Researchers are actively exploring a multitude of approaches, often drawing
inspiration from diverse disciplines including neuroscience, cognitive
science, and philosophy.146 This reflects the inherent complexity of
intelligence and understanding, and the ongoing effort to develop a
comprehensive theoretical foundation for artificial intelligence. It is likely
that moving beyond the limitations of pattern matching to achieve genuine
understanding in AI will require the integration of multiple theoretical
concepts and approaches. This might involve combining the strengths of neural
networks in pattern recognition with the logical inference capabilities of
symbolic reasoning, or incorporating principles of embodied cognition to
ground AI's understanding in interaction with the world.57 Pattern matching
alone appears insufficient to replicate the depth and flexibility of human
understanding. Therefore, a more holistic approach that incorporates
reasoning, continuous learning, and a richer, more nuanced representation of
the world is likely to be necessary. Continued development and refinement of
these theoretical frameworks are essential for guiding future research in AI
and for establishing a solid foundation upon which truly intelligent and
understanding artificial systems can be built. A strong theoretical
underpinning will be crucial for addressing the fundamental challenges in
creating AI that can not only perform tasks but also comprehend the world and
its own place within it in a meaningful way.

## Conclusion

Research on emergent cognitive architectures in advanced AI is a rapidly
evolving field with significant implications for the future of artificial
intelligence. The phenomenon of emergent abilities in large language models,
while demonstrating the power of scaling, continues to be debated regarding
its fundamental nature and predictability. The quest for self-understanding in
AI draws inspiration from human consciousness and explores various mechanisms
such as embodiment, metacognition, and internal models, yet faces profound
philosophical and technical challenges. Principled reasoning, encompassing
analogical, abductive, counterfactual, moral, and causal forms, is crucial for
building reliable and trustworthy AI systems, but achieving human-level
sophistication across these modalities remains an open challenge. Mechanistic
interpretability offers a promising approach to understanding the inner
workings of AI models, providing insights beyond black-box methods, although
scalability and automation remain key hurdles. These research areas are deeply
intertwined with the development of Artificial General Intelligence and the
critical need to ensure the safety and alignment of advanced AI systems with
human values. The evaluation of cognitive capabilities in AI is an ongoing
process, with the AI community continuously developing new and more
challenging benchmarks to overcome the limitations of existing ones and to
better measure true understanding rather than just pattern matching. Finally,
the development of robust theoretical frameworks for artificial understanding,
drawing from diverse disciplines, is essential for guiding future research and
building AI systems that can comprehend the world in a meaningful and
generalizable way. Continued in-depth research, multi-layered insights, and a
dedicated effort to address the complex challenges outlined in this report
will be necessary to advance the field towards the development of
sophisticated and beneficial advanced AI systems.

Table 1: Summary of Different Types of AI Reasoning

Type of Reasoning| Definition| Key Characteristics| Potential Applications|
Key Challenges\
---|---|---|---|---\
Analogical| Solving new problems by drawing parallels to known scenarios.|
Transfers knowledge based on structural similarities.| Problem-solving,
learning, knowledge transfer.| Requires flexible representation; LLMs may
mimic rather than reason.\
Abductive| Inferring the most plausible explanation for observations with
incomplete information.| Focuses on likelihood and relevance; combines
probabilities and domain knowledge.| Medical diagnosis, fault detection,
natural language understanding.| Reliance on knowledge quality; difficult
algorithmic implementation.\
Counterfactual| Reasoning about alternative past events to understand
causality.| Explores "what if" scenarios; identifies influential input
variables.| Model debugging, fairness analysis, regulatory compliance.|
Ensuring realism and plausibility of scenarios.\
Moral| Making ethical judgments based on moral principles.| Guided by
principles (top-down) or learned from examples (bottom-up).| Ethical decision-
making in autonomous systems.| Defining ethical principles; avoiding biases.\
Causal| Understanding and modeling cause-and-effect relationships.| Goes
beyond correlation; uses causal graphs and interventions.| Product
development, marketing, healthcare, finance.| Modeling complex real-world
causality.

Table 2: Key Benchmarks for Evaluating AI Reasoning

Benchmark Name| Focus Area| Key Characteristics/Tasks| Reported Performance of
State-of-the-Art Models (Example)| Limitations\
---|---|---|---|---\
MMLU| Language Understanding| Covers 57 subjects; tests broad knowledge.|
Phi-3-mini achieves >60%| May be becoming saturated.\
GSM8K| Mathematical Reasoning| Grade-school math problems.| -| -\
ARC| Abstract Reasoning| Visual puzzles requiring pattern recognition and
generalization.| -| Text-only models may be disadvantaged.\
FrontierMath| Mathematical Reasoning| Expert-level, unpublished mathematics
problems.| AI systems solve only 2%| Very challenging.\
Humanity's Last Exam| General Academic| Rigorous academic test across multiple
domains.| Top system scores just 8.80%| Very challenging.\
BigCodeBench| Coding| Tests code generation and problem-solving.| AI systems
achieve 35.5% (Human: 97%)| Significant gap between AI and human performance.\
GameArena| Reasoning (Interactive)| Live computer games (Akinator, Taboo,
Bluffing) testing various reasoning types.| -| Relatively new; further
research needed.\
FLIP| Multimodal Reasoning| Ordering of 4 images to identify a logically
coherent sequence based on human verification.| Gemini 1.5 Pro: 77.9% (Human:
95.3%)| Highlights limitations of current models.\
Turing Applied AGI...| Real-World Knowledge Work| Tasks in software
engineering, data science, math, and multimodal reasoning.| -| Focus on
practical applications.\
MathR-Eval| Mathematical Reasoning| 100 logical mathematics questions (not
advanced calculus).| o1 and o3-mini perform best.| Focus on logical reasoning.\
PlanBench| Planning and Reasoning| Series of planning tasks testing step-by-
step plan generation for specific goals.| o1 shows high accuracy but high
cost.| Efficiency, cost, and transparency are challenges.\
GSM-Symbolic| Mathematical Reasoning| Builds on GSM8K with symbolic templates
to test logical capabilities and robustness to irrelevant information.|
Significant performance drops with irrelevant info.| Suggests LLMs may mimic
rather than reason logically.

#### Works cited

1. Emergent Abilities of Large Language Models - OpenReview, accessed on May 17, 2025, [https://openreview.net/pdf?id=yzkSU5zdwD](https://openreview.net/pdf?id=yzkSU5zdwD)

1. Emergent abilities of large language models - Google Research, accessed on May 17, 2025, [https://research.google/pubs/emergent-abilities-of-large-language-models/](https://research.google/pubs/emergent-abilities-of-large-language-models/)

1. (PDF) AI and the Cognitive Sense of Self - ResearchGate, accessed on May 17, 2025, [https://www.researchgate.net/publication/388274949_AI_and_the_Cognitive_Sense_of_Self](https://www.researchgate.net/publication/388274949_AI_and_the_Cognitive_Sense_of_Self)

1. Exploring the Cognitive Sense of Self in AI: Ethical Frameworks and Technological Advances for Enhanced Decision-Making - Digital Commons@Lindenwood University, accessed on May 17, 2025, [https://digitalcommons.lindenwood.edu/faculty-research-papers/715/](https://digitalcommons.lindenwood.edu/faculty-research-papers/715/)

1. Towards Self-Aware AI: Embodiment, Feedback Loops, and the ..., accessed on May 17, 2025, [https://www.preprints.org/manuscript/202411.0661/v1](https://www.preprints.org/manuscript/202411.0661/v1)

1. What are the different types of reasoning in AI? - Milvus, accessed on May 17, 2025, [https://milvus.io/ai-quick-reference/what-are-the-different-types-of-reasoning-in-ai](https://milvus.io/ai-quick-reference/what-are-the-different-types-of-reasoning-in-ai)

1. What is Reasoning in AI? Types and Applications in 2025 - Aisera, accessed on May 17, 2025, [https://aisera.com/blog/ai-reasoning/](https://aisera.com/blog/ai-reasoning/)

1. The Rise of Reasoning Models: How AI is Learning to Think Step by Step - Hiflylabs, accessed on May 17, 2025, [https://hiflylabs.com/blog/2025/4/3/reasoning-models](https://hiflylabs.com/blog/2025/4/3/reasoning-models)

1. Emergent Abilities in Large Language Models: A Survey - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2503.05788v2](https://arxiv.org/html/2503.05788v2)

1. Is AI Safe? Understanding the Risks of Artificial General Intelligence, accessed on May 17, 2025, [https://technorizen.com/is-ai-safe-understanding-the-risks-of-artificial-general-intelligence/](https://technorizen.com/is-ai-safe-understanding-the-risks-of-artificial-general-intelligence/)

1. proceedings.neurips.cc, accessed on May 17, 2025, [https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403b8311ca7e8bd7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403b8311ca7e8bd7-Paper-Conference.pdf)

1. Emergent Abilities in Large Language Models: An Explainer - CSET, accessed on May 17, 2025, [https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/)

1. AI's Ostensible Emergent Abilities Are a Mirage | Stanford HAI, accessed on May 17, 2025, [https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage](https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage)

1. [2304.15004] Are Emergent Abilities of Large Language Models a Mirage? - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004)

1. Examining Emergent Abilities in Large Language ... - Stanford HAI, accessed on May 17, 2025, [https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models](https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models)

1. [2206.07682] Emergent Abilities of Large Language Models - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)

1. Emergent Abilities of Large Language Models - OpenReview, accessed on May 17, 2025, [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD)

1. Emergent Abilities in Large Language Models: A Survey - arXiv, accessed on May 17, 2025, [https://arxiv.org/pdf/2503.05788](https://arxiv.org/pdf/2503.05788)

1. Harnessing Metacognition for Safe and Responsible AI - MDPI, accessed on May 17, 2025, [https://www.mdpi.com/2227-7080/13/3/107](https://www.mdpi.com/2227-7080/13/3/107)

1. Imagining and building wise machines: The centrality of AI metacognition - Causality in Cognition Lab, accessed on May 17, 2025, [https://cicl.stanford.edu/papers/johnson2024wise.pdf](https://cicl.stanford.edu/papers/johnson2024wise.pdf)

1. Imagining and building wise machines: The centrality of AI metacognition - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2411.02478](https://arxiv.org/abs/2411.02478)

1. Analyzing internal world models of humans, animals and AI | ScienceDaily, accessed on May 17, 2025, [https://www.sciencedaily.com/releases/2024/07/240718124848.htm](https://www.sciencedaily.com/releases/2024/07/240718124848.htm)

1. I-JEPA: The first AI model based on Yann LeCun's vision for more human-like AI, accessed on May 17, 2025, [https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/)

1. [2306.17258] Suffering Toasters -- A New Self-Awareness Test for AI - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2306.17258](https://arxiv.org/abs/2306.17258)

1. Thinking Fast and Slow in AI: the Role of Metacognition, accessed on May 17, 2025, [https://www.loreggia.eu/MetacogNeurIPS2021/MADL2021_paper_3.pdf](https://www.loreggia.eu/MetacogNeurIPS2021/MADL2021_paper_3.pdf)

1. Self-Awareness, a Singularity of AI - David Publishing Company, accessed on May 17, 2025, [https://www.davidpublisher.com/Public/uploads/Contribute/6454a6a738fa1.pdf](https://www.davidpublisher.com/Public/uploads/Contribute/6454a6a738fa1.pdf)

1. Artificial General Intelligence - The Decision Lab, accessed on May 17, 2025, [https://thedecisionlab.com/reference-guide/computer-science/artificial-general-intelligence](https://thedecisionlab.com/reference-guide/computer-science/artificial-general-intelligence)

1. [D] Neural nets are not "slightly conscious," and AI PR can do with less hype - Reddit, accessed on May 17, 2025, [https://www.reddit.com/r/MachineLearning/comments/sxaiq8/d_neural_nets_are_not_slightly_conscious_and_ai/](https://www.reddit.com/r/MachineLearning/comments/sxaiq8/d_neural_nets_are_not_slightly_conscious_and_ai/)

1. The Role of Analogical Reasoning in Advancing Artificial Intelligence - Trailyn Ventures, accessed on May 17, 2025, [https://www.trailyn.com/the-role-of-analogical-reasoning-in-advancing-artificial-intelligence/](https://www.trailyn.com/the-role-of-analogical-reasoning-in-advancing-artificial-intelligence/)

1. LLMs as Models for Analogical Reasoning - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2406.13803v2](https://arxiv.org/html/2406.13803v2)

1. Learning to Make Analogies by Contrasting Abstract Relational Structure - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/1902.00120](https://arxiv.org/abs/1902.00120)

1. LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT RELATIONAL STRUCTURE, accessed on May 17, 2025, [https://web.stanford.edu/class/cs379c/class_messages_listing/curriculum/Annotated_Readings/HilletalICLR-19_Unannotated.pdf](https://web.stanford.edu/class/cs379c/class_messages_listing/curriculum/Annotated_Readings/HilletalICLR-19_Unannotated.pdf)

1. Learning to Make Analogies by Contrasting Abstract Relational Structure - OpenReview, accessed on May 17, 2025, [https://openreview.net/forum?id=SylLYsCcFm](https://openreview.net/forum?id=SylLYsCcFm)

1. Neural Analogical Reasoning - CEUR-WS.org, accessed on May 17, 2025, [https://ceur-ws.org/Vol-3212/paper9.pdf](https://ceur-ws.org/Vol-3212/paper9.pdf)

1. Analogical Reasoning With Deep Learning-Based Symbolic Processing - Semantic Scholar, accessed on May 17, 2025, [https://www.semanticscholar.org/paper/Analogical-Reasoning-With-Deep-Learning-Based-Honda-Hagiwara/fee0c47bef9ab8803061019026516c008b6edddf](https://www.semanticscholar.org/paper/Analogical-Reasoning-With-Deep-Learning-Based-Honda-Hagiwara/fee0c47bef9ab8803061019026516c008b6edddf)

1. Abstraction and analogyâ€making in artificial intelligence - Semantic Scholar, accessed on May 17, 2025, [https://www.semanticscholar.org/paper/Abstraction-and-analogy%E2%80%90making-in-artificial-Mitchell/8c479e81ddaf55aba9044449b5be7b7bf2046b7e](https://www.semanticscholar.org/paper/Abstraction-and-analogy%E2%80%90making-in-artificial-Mitchell/8c479e81ddaf55aba9044449b5be7b7bf2046b7e)

1. (PDF) Analogical Reasoning With Deep Learning-Based Symbolic Processing, accessed on May 17, 2025, [https://www.researchgate.net/publication/354264584_Analogical_Reasoning_With_Deep_Learning-Based_Symbolic_Processing](https://www.researchgate.net/publication/354264584_Analogical_Reasoning_With_Deep_Learning-Based_Symbolic_Processing)

1. Apple AI researchers question OpenAI's claims about o1's reasoning capabilities [about paper "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"] : r/singularity - Reddit, accessed on May 17, 2025, [https://www.reddit.com/r/singularity/comments/1g1zphu/apple_ai_researchers_question_openais_claims/](https://www.reddit.com/r/singularity/comments/1g1zphu/apple_ai_researchers_question_openais_claims/)

1. How does abductive reasoning work in AI? - Milvus, accessed on May 17, 2025, [https://milvus.io/ai-quick-reference/how-does-abductive-reasoning-work-in-ai](https://milvus.io/ai-quick-reference/how-does-abductive-reasoning-work-in-ai)

1. Exploring abductive reasoning in AI - IndiaAI, accessed on May 17, 2025, [https://indiaai.gov.in/article/exploring-abductive-reasoning-in-ai](https://indiaai.gov.in/article/exploring-abductive-reasoning-in-ai)

1. Is abductive reasoning, and the inferences it draws through having familiarity with a way of life or specific contextual practices, something external to the way that artificial intelligence operates or could potentially operate? : r/askphilosophy - Reddit, accessed on May 17, 2025, [https://www.reddit.com/r/askphilosophy/comments/1aj6sar/is_abductive_reasoning_and_the_inferences_it/](https://www.reddit.com/r/askphilosophy/comments/1aj6sar/is_abductive_reasoning_and_the_inferences_it/)

1. Counterfactual Explanations: The What-Ifs of AI Decision Making, accessed on May 17, 2025, [https://kpmg.com/ch/en/insights/artificial-intelligence/counterfactual-explanation.html](https://kpmg.com/ch/en/insights/artificial-intelligence/counterfactual-explanation.html)

1. Viewing the process of generating counterfactuals as a source of knowledge: a new approach for explaining classifiers - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2309.04284v4](https://arxiv.org/html/2309.04284v4)

1. Humans Use Counterfactuals to Reason About Causality. Can AI? | Stanford HAI, accessed on May 17, 2025, [https://hai.stanford.edu/news/humans-use-counterfactuals-reason-about-causality-can-ai](https://hai.stanford.edu/news/humans-use-counterfactuals-reason-about-causality-can-ai)

1. Counterfactuals in Language AI - Towards Data Science, accessed on May 17, 2025, [https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64/](https://towardsdatascience.com/counterfactuals-in-language-ai-956673049b64/)

1. Are Artificial Moral Agents the Future of Ethical AI? | Tepperspectives, accessed on May 17, 2025, [https://tepperspectives.cmu.edu/all-articles/are-artificial-moral-agents-the-future-of-ethical-ai/](https://tepperspectives.cmu.edu/all-articles/are-artificial-moral-agents-the-future-of-ethical-ai/)

1. Inducing human-like biases in moral reasoning LMs - LessWrong, accessed on May 17, 2025, [https://www.lesswrong.com/posts/eruHcdS9DmQsgLqd4/inducing-human-like-biases-in-moral-reasoning-lms](https://www.lesswrong.com/posts/eruHcdS9DmQsgLqd4/inducing-human-like-biases-in-moral-reasoning-lms)

1. How AI tools canâ€”and cannotâ€”help organizations become more ethical - PMC, accessed on May 17, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10324517/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10324517/)

1. Full article: Artificial intelligence as a moral mentor - Taylor & Francis Online, accessed on May 17, 2025, [https://www.tandfonline.com/doi/full/10.1080/03057240.2025.2475539?src=](https://www.tandfonline.com/doi/full/10.1080/03057240.2025.2475539?src)

1. Large Concept Models: A Paradigm Shift in AI Reasoning - InfoQ, accessed on May 17, 2025, [https://www.infoq.com/articles/lcm-paradigm-shift-ai-reasoning/](https://www.infoq.com/articles/lcm-paradigm-shift-ai-reasoning/)

1. Causal AI: The Future of Intelligent Decision-Making - Kanerika, accessed on May 17, 2025, [https://kanerika.com/blogs/causal-ai/](https://kanerika.com/blogs/causal-ai/)

1. What is Causal AI? Understanding Causes and Effects - DataCamp, accessed on May 17, 2025, [https://www.datacamp.com/blog/what-is-causal-ai](https://www.datacamp.com/blog/what-is-causal-ai)

1. Why Causal AI? | causaLens, accessed on May 17, 2025, [https://causalai.causalens.com/why-causal-ai/](https://causalai.causalens.com/why-causal-ai/)

1. CausalML Book, accessed on May 17, 2025, [https://causalml-book.org/](https://causalml-book.org/)

1. Causal AI: the revolution uncovering the 'why' of decision-making | World Economic Forum, accessed on May 17, 2025, [https://www.weforum.org/stories/2024/04/causal-ai-decision-making/](https://www.weforum.org/stories/2024/04/causal-ai-decision-making/)

1. What Is Reasoning in AI? | IBM, accessed on May 17, 2025, [https://www.ibm.com/think/topics/ai-reasoning](https://www.ibm.com/think/topics/ai-reasoning)

1. What is AI reasoning in 2025? | AI reasoning and problem solving | Knowledge and reasoning in AI - Lumenalta, accessed on May 17, 2025, [https://lumenalta.com/insights/what-is-ai-reasoning-in-2025](https://lumenalta.com/insights/what-is-ai-reasoning-in-2025)

1. Reasoning Engine: Your Guide to Intelligent Decision-Making Systems | Guru, accessed on May 17, 2025, [https://www.getguru.com/reference/reasoning-engine](https://www.getguru.com/reference/reasoning-engine)

1. Advancements in AI for Reasoning with Complex Data | Proceedings of the AAAI Conference on Artificial Intelligence, accessed on May 17, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/35106](https://ojs.aaai.org/index.php/AAAI/article/view/35106)

1. Mechanistic? - OpenReview, accessed on May 17, 2025, [https://openreview.net/notes/edits/attachment?id=fKPWwcax37&name=pdf](https://openreview.net/notes/edits/attachment?id=fKPWwcax37&name=pdf)

1. openreview.net, accessed on May 17, 2025, [https://openreview.net/pdf?id=91H76m9Z94](https://openreview.net/pdf?id=91H76m9Z94)

1. Mechanistic Interpretability for AI Safety A Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2404.14082v1](https://arxiv.org/html/2404.14082v1)

1. [2404.14082] Mechanistic Interpretability for AI Safety -- A Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2404.14082](https://arxiv.org/abs/2404.14082)

1. Mechanistic Interpretability for AI Safety â€” A Review | Leonard F. Bereska, accessed on May 17, 2025, [https://leonardbereska.github.io/blog/2024/mechinterpreview/](https://leonardbereska.github.io/blog/2024/mechinterpreview/)

1. Mechanistic Interpretability for AI Safety A Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2404.14082v2](https://arxiv.org/html/2404.14082v2)

1. Mechanistic Interpretability for AI Safety A Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2404.14082v3](https://arxiv.org/html/2404.14082v3)

1. Mechanistic Interpretability for AI Safety - A Review - OpenReview, accessed on May 17, 2025, [https://openreview.net/forum?id=ePUVetPKu6](https://openreview.net/forum?id=ePUVetPKu6)

1. [PDF] Mechanistic Interpretability for AI Safety--A Review Combinational regularity analysis (CORA), accessed on May 17, 2025, [http://web.cs.ucla.edu/~kaoru/google4-29-2024.pdf](http://web.cs.ucla.edu/~kaoru/google4-29-2024.pdf)

1. [R] Has Explainable AI Research Tanked? : r/MachineLearning - Reddit, accessed on May 17, 2025, [https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/](https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/)

1. Mechanistic Interpretability for AI Safety A Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2404.14082](https://arxiv.org/html/2404.14082)

1. Mechanistic Interpretability - Aussie AI, accessed on May 17, 2025, [https://www.aussieai.com/research/mechanistic-interpretability](https://www.aussieai.com/research/mechanistic-interpretability)

1. Mechanistic Interpretability for AI Safety A Review | OpenReview, accessed on May 17, 2025, [https://openreview.net/pdf/ea3c9a4135caad87031d3e445a80d0452f83da5d.pdf](https://openreview.net/pdf/ea3c9a4135caad87031d3e445a80d0452f83da5d.pdf)

1. Mechanistic Interpretability Via Learning Differential Equations: AI Safety Camp Project Intermediate Report. - LessWrong, accessed on May 17, 2025, [https://www.lesswrong.com/posts/qdxNsbY5kYNqcgzFb/mechanistic-interpretability-via-learning-differential](https://www.lesswrong.com/posts/qdxNsbY5kYNqcgzFb/mechanistic-interpretability-via-learning-differential)

1. Inside AI's Black Box: Mechanistic Interpretability as a Key to AI Transparency, accessed on May 17, 2025, [https://community.datascience.hp.com/artificial-intelligence-62/inside-ai-s-black-box-mechanistic-interpretability-as-a-key-to-ai-transparency-274](https://community.datascience.hp.com/artificial-intelligence-62/inside-ai-s-black-box-mechanistic-interpretability-as-a-key-to-ai-transparency-274)

1. Takeaways from the Mechanistic Interpretability Challenges - AI Alignment Forum, accessed on May 17, 2025, [https://www.alignmentforum.org/posts/EjsA2M8p8ERyFHLLY/takeaways-from-the-mechanistic-interpretability-challenges](https://www.alignmentforum.org/posts/EjsA2M8p8ERyFHLLY/takeaways-from-the-mechanistic-interpretability-challenges)

1. [2407.02646] A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2407.02646](https://arxiv.org/abs/2407.02646)

1. A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models - OpenReview, accessed on May 17, 2025, [https://openreview.net/forum?id=xwoTdKr0rM](https://openreview.net/forum?id=xwoTdKr0rM)

1. Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2412.02104](https://arxiv.org/html/2412.02104)

1. A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models - arXiv, accessed on May 17, 2025, [https://arxiv.org/html/2502.17516v1](https://arxiv.org/html/2502.17516v1)

1. [2502.17516] A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models, accessed on May 17, 2025, [https://arxiv.org/abs/2502.17516](https://arxiv.org/abs/2502.17516)

1. [Literature Review] A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models - Moonlight, accessed on May 17, 2025, [https://www.themoonlight.io/en/review/a-survey-on-mechanistic-interpretability-for-multi-modal-foundation-models](https://www.themoonlight.io/en/review/a-survey-on-mechanistic-interpretability-for-multi-modal-foundation-models)

1. (PDF) Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey - ResearchGate, accessed on May 17, 2025, [https://www.researchgate.net/publication/386419014_Explainable_and_Interpretable_Multimodal_Large_Language_Models_A_Comprehensive_Survey](https://www.researchgate.net/publication/386419014_Explainable_and_Interpretable_Multimodal_Large_Language_Models_A_Comprehensive_Survey)

1. itsqyh/Awesome-LMMs-Mechanistic-Interpretability - GitHub, accessed on May 17, 2025, [https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability](https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability)

1. A Survey on Mechanistic Interpretability for Multi-Modal Foundation, accessed on May 17, 2025, [https://openreview.net/pdf/7b15cbcf0e6eb7f970a7346ada0d3cea572203e1.pdf](https://openreview.net/pdf/7b15cbcf0e6eb7f970a7346ada0d3cea572203e1.pdf)

1. LLM4RO/README.md at main Â· xianchaoxiu/LLM4RO Â· GitHub, accessed on May 17, 2025, [https://github.com/xianchaoxiu/LLM4RO/blob/main/README.md](https://github.com/xianchaoxiu/LLM4RO/blob/main/README.md)

1. Hongxuan Li's research works - ResearchGate, accessed on May 17, 2025, [https://www.researchgate.net/scientific-contributions/Hongxuan-Li-2293029294](https://www.researchgate.net/scientific-contributions/Hongxuan-Li-2293029294)

1. A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models. - DBLP, accessed on May 17, 2025, [https://dblp.org/rec/journals/corr/abs-2502-17516](https://dblp.org/rec/journals/corr/abs-2502-17516)

1. Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems - LessWrong, accessed on May 17, 2025, [https://www.lesswrong.com/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic](https://www.lesswrong.com/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic)

1. Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems - AI Alignment Forum, accessed on May 17, 2025, [https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic](https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic)

1. Mohammad Beigi - Papers With Code, accessed on May 17, 2025, [https://paperswithcode.com/author/mohammad-beigi](https://paperswithcode.com/author/mohammad-beigi)

1. [2501.16496] Open Problems in Mechanistic Interpretability - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2501.16496](https://arxiv.org/abs/2501.16496)

1. Paper: Open Problems in Mechanistic Interpretability - LessWrong, accessed on May 17, 2025, [https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability](https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability)

1. Capabilities and alignment of LLM cognitive architectures - AI Alignment Forum, accessed on May 17, 2025, [https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures](https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures)

1. AGI vs AI: Key Differences & Future Implications - Zignuts Technolab, accessed on May 17, 2025, [https://www.zignuts.com/blog/agi-vs-ai-differences](https://www.zignuts.com/blog/agi-vs-ai-differences)

1. Overview of Emergent and Novel Behavior in AI Systems | Center for ..., accessed on May 17, 2025, [https://www.centeraipolicy.org/work/emergence-overview](https://www.centeraipolicy.org/work/emergence-overview)

1. "Magical" Emergent Behaviours in AI: A Security Perspective, accessed on May 17, 2025, [https://securing.ai/ai-security/emergent-behaviors-ai-security/](https://securing.ai/ai-security/emergent-behaviors-ai-security/)

1. AI alignment - Wikipedia, accessed on May 17, 2025, [https://en.wikipedia.org/wiki/AI_alignment](https://en.wikipedia.org/wiki/AI_alignment)

1. Thoughts on AGI safety from the top - AI Alignment Forum, accessed on May 17, 2025, [https://www.alignmentforum.org/posts/ApLnWjgMwBTJt6buC/thoughts-on-agi-safety-from-the-top](https://www.alignmentforum.org/posts/ApLnWjgMwBTJt6buC/thoughts-on-agi-safety-from-the-top)

1. AGI safety from first principles: Introduction - AI Alignment Forum, accessed on May 17, 2025, [https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction](https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction)

1. Core Views on AI Safety: When, Why, What, and How \\ Anthropic, accessed on May 17, 2025, [https://www.anthropic.com/news/core-views-on-ai-safety](https://www.anthropic.com/news/core-views-on-ai-safety)

1. Forecasting emergent risks in advanced AI systems: an analysis of a future road transport management system - PubMed, accessed on May 17, 2025, [https://pubmed.ncbi.nlm.nih.gov/38009364/](https://pubmed.ncbi.nlm.nih.gov/38009364/)

1. Why Uncontrollable AI Looks More Likely Than Ever | TIME, accessed on May 17, 2025, [https://time.com/6258483/uncontrollable-ai-agi-risks/](https://time.com/6258483/uncontrollable-ai-agi-risks/)

1. AI Risks that Could Lead to Catastrophe | CAIS - Center for AI Safety, accessed on May 17, 2025, [https://www.safe.ai/ai-risk](https://www.safe.ai/ai-risk)

1. Navigating Artificial General Intelligence (AGI): Societal Implications, Ethical Considerations, and Governance Strategies - Preprints.org, accessed on May 17, 2025, [https://www.preprints.org/manuscript/202407.1573/v3](https://www.preprints.org/manuscript/202407.1573/v3)

1. Implications of Artificial General Intelligence on National and International Security, accessed on May 17, 2025, [https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/](https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/)

1. The Anthropocentric Mirror: Examining Bias, Consequences, and Alternatives in Artificial Intelligence Development - Alphanome.AI, accessed on May 17, 2025, [https://www.alphanome.ai/post/the-anthropocentric-mirror-examining-bias-consequences-and-alternatives-in-artificial-intelligenc](https://www.alphanome.ai/post/the-anthropocentric-mirror-examining-bias-consequences-and-alternatives-in-artificial-intelligenc)

1. Examining AI Safety as a Global Public Good: Implications - Oxford Martin School, accessed on May 17, 2025, [https://www.oxfordmartin.ox.ac.uk/publications/examining-ai-safety-as-a-global-public-good-implications-challenges-and-research-priorities](https://www.oxfordmartin.ox.ac.uk/publications/examining-ai-safety-as-a-global-public-good-implications-challenges-and-research-priorities)

1. Towards a Comprehensive Theory of Aligned Emergence in AI Systems: Navigating Complexity towards Coherence - Qeios, accessed on May 17, 2025, [https://www.qeios.com/read/1OHD8T](https://www.qeios.com/read/1OHD8T)

1. What are common benchmarks for AI reasoning? - Milvus, accessed on May 17, 2025, [https://milvus.io/ai-quick-reference/what-are-common-benchmarks-for-ai-reasoning](https://milvus.io/ai-quick-reference/what-are-common-benchmarks-for-ai-reasoning)

1. Best Benchmarks for Evaluating LLMs' Critical Thinking Abilities - Galileo AI, accessed on May 17, 2025, [https://www.galileo.ai/blog/best-benchmarks-for-evaluating-llms-critical-thinking-abilities](https://www.galileo.ai/blog/best-benchmarks-for-evaluating-llms-critical-thinking-abilities)

1. EXAONE Deep Released â” Setting a New Standard for Reasoning AI - LG AIì—°êµ¬ì›, accessed on May 17, 2025, [https://www.lgresearch.ai/news/view?seq=543](https://www.lgresearch.ai/news/view?seq=543)

1. Neural Structure Mapping For Learning Abstract Visual Analogies - OpenReview, accessed on May 17, 2025, [https://openreview.net/forum?id=By5Uwd_xzNF](https://openreview.net/forum?id=By5Uwd_xzNF)

1. Technical Performance | The 2025 AI Index Report | Stanford HAI, accessed on May 17, 2025, [https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)

1. Introducing OpenAI o3 and o4-mini, accessed on May 17, 2025, [https://openai.com/index/introducing-o3-and-o4-mini/](https://openai.com/index/introducing-o3-and-o4-mini/)

1. Challenges in evaluating AI systems - Anthropic, accessed on May 17, 2025, [https://www.anthropic.com/research/evaluating-ai-systems](https://www.anthropic.com/research/evaluating-ai-systems)

1. Introducing Real-World AI Benchmarks for AGI Progress - Turing, accessed on May 17, 2025, [https://www.turing.com/blog/rethinking-ai-benchmarks-for-real-world-impact](https://www.turing.com/blog/rethinking-ai-benchmarks-for-real-world-impact)

1. The Limits of AI Reasoning: Beyond the Illusion of Intelligence - VE3, accessed on May 17, 2025, [https://www.ve3.global/the-limits-of-ai-reasoning-beyond-the-illusion-of-intelligence/](https://www.ve3.global/the-limits-of-ai-reasoning-beyond-the-illusion-of-intelligence/)

1. Unveiling the AI Illusion: Why Chatbots Lack True Understanding and Intelligence, accessed on May 17, 2025, [https://ai-cosmos.hashnode.dev/unveiling-the-ai-illusion-why-chatbots-lack-true-understanding-and-intelligence](https://ai-cosmos.hashnode.dev/unveiling-the-ai-illusion-why-chatbots-lack-true-understanding-and-intelligence)

1. Toward human-level concept learning: Pattern benchmarking for AI algorithms - PMC, accessed on May 17, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10435961/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10435961/)

1. [R] Do people still believe in LLM emergent abilities? : r/MachineLearning - Reddit, accessed on May 17, 2025, [https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/](https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/)

1. FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI, accessed on May 17, 2025, [https://epoch.ai/frontiermath/the-benchmark](https://epoch.ai/frontiermath/the-benchmark)

1. How to evaluate the reasoning capabilities of LLMs in a more dynamic scenario - Medium, accessed on May 17, 2025, [https://medium.com/about-ai/how-to-evaluate-the-reasoning-capabilities-of-llms-in-a-more-dynamic-scenario-a7ed766afde0](https://medium.com/about-ai/how-to-evaluate-the-reasoning-capabilities-of-llms-in-a-more-dynamic-scenario-a7ed766afde0)

1. [2504.12256] FLIP Reasoning Challenge - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2504.12256](https://arxiv.org/abs/2504.12256)

1. AI Reasoning Benchmark: MathR-Eval in 2025 - Research AIMultiple, accessed on May 17, 2025, [https://research.aimultiple.com/ai-reasoning/](https://research.aimultiple.com/ai-reasoning/)

1. Advancements in AI Planning: OpenAI's o1 and Large Reasoning Models (LRMs), accessed on May 17, 2025, [https://ajithp.com/2024/09/30/ai-reasoning-and-lrms/](https://ajithp.com/2024/09/30/ai-reasoning-and-lrms/)

1. RAG Evolution with Reasoning Models - OpenAI Developer Forum, accessed on May 17, 2025, [https://community.openai.com/t/rag-evolution-with-reasoning-models/1232802](https://community.openai.com/t/rag-evolution-with-reasoning-models/1232802)

1. Approaches for monitoring quality of reasoning capabilities in production - API, accessed on May 17, 2025, [https://community.openai.com/t/approaches-for-monitoring-quality-of-reasoning-capabilities-in-production/695313](https://community.openai.com/t/approaches-for-monitoring-quality-of-reasoning-capabilities-in-production/695313)

1. A theory of understanding for artificial intelligence: composability, catalysts, and learning, accessed on May 17, 2025, [https://arxiv.org/html/2408.08463v1](https://arxiv.org/html/2408.08463v1)

1. [2408.08463] A theory of understanding for artificial intelligence: composability, catalysts, and learning - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2408.08463](https://arxiv.org/abs/2408.08463)

1. [Literature Review] A theory of understanding for artificial intelligence: composability, catalysts, and learning - Moonlight | AI Colleague for Research Papers, accessed on May 17, 2025, [https://www.themoonlight.io/en/review/a-theory-of-understanding-for-artificial-intelligence-composability-catalysts-and-learning](https://www.themoonlight.io/en/review/a-theory-of-understanding-for-artificial-intelligence-composability-catalysts-and-learning)

1. Artificial understanding: a step toward robust AI | Request PDF - ResearchGate, accessed on May 17, 2025, [https://www.researchgate.net/publication/369265890_Artificial_understanding_a_step_toward_robust_AI](https://www.researchgate.net/publication/369265890_Artificial_understanding_a_step_toward_robust_AI)

1. From internal models toward metacognitive AI - PMC - PubMed Central, accessed on May 17, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8551129/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8551129/)

1. Exploring Symbolic AI and Ontologies: Enhancing ... - SmythOS, accessed on May 17, 2025, [https://smythos.com/ai-agents/agent-architectures/symbolic-ai-and-ontologies/](https://smythos.com/ai-agents/agent-architectures/symbolic-ai-and-ontologies/)

1. Ontology Learning - Lark, accessed on May 17, 2025, [https://www.larksuite.com/en_us/topics/ai-glossary/ontology-learning](https://www.larksuite.com/en_us/topics/ai-glossary/ontology-learning)

1. Ontologies. A tool supporting AI and business - MakoLab, accessed on May 17, 2025, [https://makolab.com/insights/ontologies-a-tool-supporting-ai-and-business](https://makolab.com/insights/ontologies-a-tool-supporting-ai-and-business)

1. Ontologies 101: How They Power AI and Organize Our Digital World - Shep Bryan, accessed on May 17, 2025, [https://www.shepbryan.com/blog/ontologies-101](https://www.shepbryan.com/blog/ontologies-101)

1. The Role of Ontology and Information Architecture in AI, accessed on May 17, 2025, [https://www.earley.com/insights/role-ontology-and-information-architecture-ai](https://www.earley.com/insights/role-ontology-and-information-architecture-ai)

1. Role of Ontologies in Enabling AI Transparency â€“ LFAI & Data, accessed on May 17, 2025, [https://lfaidata.foundation/blog/2023/09/29/role-of-ontologies-in-enabling-ai-transparency/](https://lfaidata.foundation/blog/2023/09/29/role-of-ontologies-in-enabling-ai-transparency/)

1. A Guide To Ontologies, The Fuel That Powers Ecommerce AI - Zoovu Blog, accessed on May 17, 2025, [https://blog.zoovu.com/what-is-an-ontology/](https://blog.zoovu.com/what-is-an-ontology/)

1. A MACHINE LEARNING ONTOLOGY - OSF, accessed on May 17, 2025, [https://osf.io/rc954/download](https://osf.io/rc954/download)

1. Combining Machine Learning and Ontology: A Systematic Literature Review - arXiv, accessed on May 17, 2025, [https://arxiv.org/abs/2401.07744](https://arxiv.org/abs/2401.07744)

1. An Introduction to Ontologies - Workera, accessed on May 17, 2025, [https://workera.ai/blog/introduction-to-ontologies](https://workera.ai/blog/introduction-to-ontologies)

1. A Framework for the Foundation of the Philosophy of Artificial Intelligence - Digital Commons@Lindenwood University, accessed on May 17, 2025, [https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1682&context=faculty-research-papers](https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1682&context=faculty-research-papers)

1. AI Consciousness: An Exploration of Possibility, Theoretical Frameworks & Challenges, accessed on May 17, 2025, [https://www.unite.ai/ai-consciousness-an-exploration-of-possibility-theoretical-frameworks-challenges/](https://www.unite.ai/ai-consciousness-an-exploration-of-possibility-theoretical-frameworks-challenges/)

1. (PDF) Theories of Artificial Intelligenceâ€”Meta-Theoretical considerations - ResearchGate, accessed on May 17, 2025, [https://www.researchgate.net/publication/303652455_Theories_of_Artificial_Intelligence-Meta-Theoretical_considerations](https://www.researchgate.net/publication/303652455_Theories_of_Artificial_Intelligence-Meta-Theoretical_considerations)

1. AI Frameworks: Top Types To Adopt in 2025 - Splunk, accessed on May 17, 2025, [https://www.splunk.com/en_us/blog/learn/ai-frameworks.html](https://www.splunk.com/en_us/blog/learn/ai-frameworks.html)

1. Theoretical Frameworks for Intelligence | The Center for Brains, Minds & Machines, accessed on May 17, 2025, [https://cbmm.mit.edu/research/thrusts/theoretical-frameworks-intelligence](https://cbmm.mit.edu/research/thrusts/theoretical-frameworks-intelligence)

1. A Theoretical Framework for Understanding Natural Language Processing within Artificial Intelligence Systems - IJFMR, accessed on May 17, 2025, [https://www.ijfmr.com/research-paper.php?id=22618](https://www.ijfmr.com/research-paper.php?id=22618)

1. Framework of Artificial Intelligence Learning Platform for Education - ERIC, accessed on May 17, 2025, [https://files.eric.ed.gov/fulltext/EJ1331125.pdf](https://files.eric.ed.gov/fulltext/EJ1331125.pdf)

1. Theoretical & Conceptual Frameworks w/ AI - YouTube, accessed on May 17, 2025, [https://m.youtube.com/watch?v=unyz4GKvNsE](https://m.youtube.com/watch?v=unyz4GKvNsE)

1. 2025: The Path Towards More Robust and Adaptable AI: A Focus on ..., accessed on May 17, 2025, [https://www.alphanome.ai/post/2025-the-path-towards-more-robust-and-adaptable-ai-a-focus-on-deep-understanding-and-generalizatio](https://www.alphanome.ai/post/2025-the-path-towards-more-robust-and-adaptable-ai-a-focus-on-deep-understanding-and-generalizatio)

\*\*
